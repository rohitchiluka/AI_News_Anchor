interface CacheEntry<T> {
  data: T;
  timestamp: number;
  expiresIn: number;
}

class AIService {
  private geminiApiKey: string;
  private cache = new Map<string, CacheEntry<string>>();
  private readonly CACHE_DURATION = 2 * 60 * 1000; // 2 minutes for AI responses

  constructor() {
    this.geminiApiKey = import.meta.env.VITE_GEMINI_API_KEY;
    
    if (!this.geminiApiKey) {
      throw new Error('VITE_GEMINI_API_KEY is required. Please add it to your .env file.');
    }
  }

  private getCacheKey(prompt: string): string {
    // Create a unique hash-like key from the entire prompt for caching
    // Use the full prompt with proper encoding to ensure uniqueness
    const fullPromptKey = encodeURIComponent(prompt.trim());
    
    // Add timestamp component to make keys more unique and prevent collisions
    const promptHash = this.simpleHash(fullPromptKey);
    
    return `ai_${promptHash}_${fullPromptKey.length}`;
  }

  private simpleHash(str: string): string {
    let hash = 0;
    if (str.length === 0) return hash.toString();
    
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // Convert to 32-bit integer
    }
    
    return Math.abs(hash).toString(36);
  }

  private isValidCache<T>(entry: CacheEntry<T>): boolean {
    return Date.now() - entry.timestamp < entry.expiresIn;
  }

  private setCache(key: string, data: string, expiresIn: number = this.CACHE_DURATION): void {
    this.cache.set(key, {
      data,
      timestamp: Date.now(),
      expiresIn,
    });
  }

  private getCache(key: string): string | null {
    const entry = this.cache.get(key);
    if (entry && this.isValidCache(entry)) {
      console.log(`AI Cache hit for key: ${key.substring(0, 20)}...`);
      return entry.data;
    }
    
    if (entry) {
      console.log(`AI Cache expired for key: ${key.substring(0, 20)}...`);
      this.cache.delete(key);
    }
    
    return null;
  }

  clearCache(): void {
    this.cache.clear();
    console.log('AI service cache cleared');
  }

  private async callGeminiApi(prompt: string): Promise<string> {
    const cacheKey = this.getCacheKey(prompt);
    
    // Check cache first
    const cachedResponse = this.getCache(cacheKey);
    if (cachedResponse) {
      return cachedResponse;
    }

    try {
      console.log('Making Gemini API call...');
      
      const geminiEndpoint = `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${this.geminiApiKey}`;

      const requestBody = {
        contents: [{
          parts: [{
            text: prompt
          }]
        }],
        generationConfig: {
          temperature: 0.7,
          topK: 40,
          topP: 0.95,
          maxOutputTokens: 1024,
        },
        safetySettings: [
          {
            category: "HARM_CATEGORY_HARASSMENT",
            threshold: "BLOCK_MEDIUM_AND_ABOVE"
          },
          {
            category: "HARM_CATEGORY_HATE_SPEECH",
            threshold: "BLOCK_MEDIUM_AND_ABOVE"
          },
          {
            category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            threshold: "BLOCK_MEDIUM_AND_ABOVE"
          },
          {
            category: "HARM_CATEGORY_DANGEROUS_CONTENT",
            threshold: "BLOCK_MEDIUM_AND_ABOVE"
          }
        ]
      };

      const response = await fetch(geminiEndpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorData = await response.text();
        console.error('Gemini API Error:', errorData);
        throw new Error(`Gemini API error (${response.status}): ${errorData}`);
      }

      const result = await response.json();
      
      // Extract the generated text from Gemini's response
      const generatedText = result.candidates?.[0]?.content?.parts?.[0]?.text;
      
      if (!generatedText) {
        console.error('No text generated by Gemini:', result);
        throw new Error('No response generated by Gemini AI');
      }

      // Cache the response
      this.setCache(cacheKey, generatedText);

      return generatedText;
    } catch (error) {
      console.error('Gemini API call failed:', error);
      throw error;
    }
  }

  async refineQuery(query: string, context?: string): Promise<string> {
    try {
      const prompt = context 
        ? `Context: ${context}\n\nPlease refine this search query to be a simple, plain-text search query suitable for news search. Use only basic keywords and avoid special characters, operators, or complex syntax. Query: "${query}"\n\nReturn only the refined search terms as plain text without quotes or special formatting.`
        : `Please refine this search query to be a simple, plain-text search query suitable for news search. Use only basic keywords and avoid special characters, operators, or complex syntax. Query: "${query}"\n\nReturn only the refined search terms as plain text without quotes or special formatting.`;
      
      const refinedQuery = await this.callGeminiApi(prompt);
      
      // Clean the response to ensure it's plain text
      const cleanedQuery = refinedQuery
        .trim()
        .replace(/['"]/g, '') // Remove quotes
        .replace(/[^\w\s]/g, ' ') // Replace special characters with spaces
        .replace(/\s+/g, ' ') // Normalize whitespace
        .trim();
      
      return cleanedQuery || query;
    } catch (error) {
      console.error('Error refining query:', error);
      // Return original query if refinement fails
      return query;
    }
  }

  async generateResponse(message: string, context?: string): Promise<string> {
    try {
      const prompt = context 
        ? `Context: ${context}\n\nUser Query: ${message}`
        : message;
      
      const response = await this.callGeminiApi(prompt);
      return response || 'I apologize, but I was unable to generate a response at this time.';
    } catch (error) {
      console.error('Error generating response:', error);
      return 'I apologize, but I encountered an error while processing your request. Please try again.';
    }
  }
}

export const aiService = new AIService();